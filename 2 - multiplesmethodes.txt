print('Computational Applied Statistics - Assignment #4 part 1 \nEtienne Bayard, 2018-05-09 \n')

# Digit learning challenge
# Use the methods taught in the course, or a good combination of the methods taught in the course to predict all 10 digits of the zipcode data. Only use ziptrain.csv data to build your model, and evaluate the accuracy of your model on ziptest.csv

# Using numpy library for data importation
import numpy as np

# Assign the filenames: file_train and file_test
path = 'C:/Users/User/Desktop/data/'
filetrain = path + 'ziptrain.csv'
filetest = path + 'ziptest.csv'

# Read the files into a numpy arrays: data_train and data_test
data_train = np.loadtxt(filetrain)
data_test = np.loadtxt(filetest)

# Define the datasets for training and testing from the imported data
X_train = data_train[:,1:257]
y_train = data_train[:,0]
X_test = data_test[:,1:257]
y_test = data_test[:,0]

# We'll do the same thing on multiple methods: train the method on the training data and then predict on the test data and assess performance

# For a quick assessment of prediction performance on the training set, we'll be using the cross validation score (k fold cross-validation k=10)
from sklearn.model_selection import cross_val_score

# For a quick assessment of prediction performance on the test set, we'll be using the accuracy score
from sklearn.metrics import accuracy_score

#To assess the computing cost of each method, we'll time the calculations:
import timeit


# Method 1: Logistic regression

#Start timer
start_lr = timeit.default_timer()

#import the logistic regression model from the Scikit-learn library and run the regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)

cvs_lr = cross_val_score(lr, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = lr.predict(X_test)
as_lr = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_lr = timeit.default_timer()
time_lr = stop_lr - start_lr


# Method 2: Linear discriminant analysis

#Start timer
start_lda = timeit.default_timer()

#import the linear discriminant model from the Scikit-learn library and run the analysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis()
lda.fit(X_train,y_train)

cvs_lda = cross_val_score(lda, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = lda.predict(X_test)
as_lda = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_lda = timeit.default_timer()
time_lda = stop_lda - start_lda


# Method 3: Quadratic discriminant analysis

#Start timer
start_qda = timeit.default_timer()

#import the quadratic discriminant model from the Scikit-learn library and run the analysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train,y_train)

cvs_qda = cross_val_score(qda, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = qda.predict(X_test)
as_qda = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_qda = timeit.default_timer()
time_qda = stop_qda - start_qda

# running the quadratic discriminant model warns about potential colinearity in variables, which is normal given the data is composed of 0 & 1 getting squared
print('Running the quadratic discriminant model warns about potential colinearity in variables, which is normal given the data is composed of 0 & 1 getting squared')

# Method 4: Bagging classifier

#Start timer
start_bag = timeit.default_timer()

#import the bagging classifier model from the Scikit-learn library and run the classification
from sklearn.ensemble import BaggingClassifier
bag = BaggingClassifier(n_estimators=100, random_state=1)
bag.fit(X_train,y_train)

cvs_bag = cross_val_score(bag, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = bag.predict(X_test)
as_bag = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_bag = timeit.default_timer()
time_bag = stop_bag - start_bag


# Method 5: ADA boost classifier

#Start timer
start_adaboost = timeit.default_timer()

#import the ADA bagging classifier model from the Scikit-learn library and run the classification
from sklearn.ensemble import AdaBoostClassifier
adaboost = AdaBoostClassifier(n_estimators=100, random_state=1)
adaboost.fit(X_train,y_train)

cvs_adaboost = cross_val_score(adaboost, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = adaboost.predict(X_test)
as_adaboost = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_adaboost = timeit.default_timer()
time_adaboost = stop_adaboost - start_adaboost


# Method 6: Gradient boosting classifier

#Start timer
start_gradboost = timeit.default_timer()

#import the gradient boosting classifier model from the Scikit-learn library and run the classification
from sklearn.ensemble import GradientBoostingClassifier
gradboost = GradientBoostingClassifier(n_estimators= 100, random_state=1)
gradboost.fit(X_train,y_train)

cvs_gradboost = cross_val_score(gradboost, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = gradboost.predict(X_test)
as_gradboost = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_gradboost = timeit.default_timer()
time_gradboost = stop_gradboost - start_gradboost


# Method 7: Random Forest

#Start timer
start_rf = timeit.default_timer()

#import the random forest model from the Scikit-learn library and run the algorithm
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train,y_train)

cvs_rf = cross_val_score(rf, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = rf.predict(X_test)
as_rf = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_rf = timeit.default_timer()
time_rf = stop_rf - start_rf



# Method 8: Support Vector Machine

#Start timer
start_sv = timeit.default_timer()

#import the support vector machine model from the Scikit-learn library and run the algorithm
from sklearn.svm import SVC
sv = SVC(C=10)
sv.fit(X_train,y_train)

cvs_sv = cross_val_score(sv, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test)  
y_test_pred = sv.predict(X_test)
as_sv = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_sv = timeit.default_timer()
time_sv = stop_sv - start_sv




# Method 9: Neural network MLP classifier

#Start timer
start_nn = timeit.default_timer()

#import the neural network MLP classifier model from the Scikit-learn library and run the classification
from sklearn.neural_network import MLPClassifier
nn = MLPClassifier(hidden_layer_sizes=(10,10), activation = 'logistic',max_iter=1000)
nn.fit(X_train,y_train)

cvs_nn = cross_val_score(nn, X_train, y_train, cv=10).mean()

# We now apply our predictive model on the test data (X_test) 
y_test_pred = nn.predict(X_test)
as_nn = accuracy_score(y_test, y_test_pred)

#stop timer and calculate computing time in seconds
stop_nn = timeit.default_timer()
time_nn = stop_nn - start_nn


#Creating a pandas dataframe summarizing this first part. First, create a dictionnary with key as the desired section and associated value
resume = [('Logistic regression', cvs_lr, as_lr, time_lr),
('Linear discriminant analysis', cvs_lda, as_lda, time_lda),
('Quadratic discriminant analysis', cvs_qda, as_qda, time_qda),
('Bagging classifier', cvs_bag, as_bag, time_bag),
('ADA boost classifier', cvs_adaboost, as_adaboost, time_adaboost),
('Gradient boosting classifier', cvs_gradboost, as_gradboost, time_gradboost),
('Random Forest', cvs_rf, as_rf, time_rf),
('Support Vector Machine', cvs_sv, as_sv, time_sv),
('Neural network MLP classifier', cvs_nn, as_nn, time_nn),]
labels = [ 'Method', 'Cross validation score', 'Accuracy score', 'Computing time (s)']

import pandas as pd
df = pd.DataFrame(resume, columns=labels)
print(df)

#Conclusion: without being too computer time expensive, the Support Vector Machine, in its default configuration, seems to be the best classifying method with a more than 95% accuracy score.
print('Without being too computer time expensive, the Support Vector Machine, in its default configuration, seems to be the best classifying method with a more than 95% accuracy score.')
