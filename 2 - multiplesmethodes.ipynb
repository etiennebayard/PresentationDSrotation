{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational Applied Statistics - Assignment #4 part 1 \n",
      "Etienne Bayard, 2018-05-09 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Computational Applied Statistics - Assignment #4 part 1 \\nEtienne Bayard, 2018-05-09 \\n')\n",
    "\n",
    "# Digit learning challenge\n",
    "# Use the methods taught in the course, or a good combination of the methods taught in the course to predict all 10 digits of the zipcode data. Only use ziptrain.csv data to build your model, and evaluate the accuracy of your model on ziptest.csv\n",
    "\n",
    "# Using numpy library for data importation\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filenames: file_train and file_test\n",
    "path = 'C:/Users/User/Desktop/data/'\n",
    "filetrain = path + 'ziptrain.csv'\n",
    "filetest = path + 'ziptest.csv'\n",
    "\n",
    "# Read the files into a numpy arrays: data_train and data_test\n",
    "data_train = np.loadtxt(filetrain)\n",
    "data_test = np.loadtxt(filetest)\n",
    "\n",
    "# Define the datasets for training and testing from the imported data\n",
    "X_train = data_train[:,1:257]\n",
    "y_train = data_train[:,0]\n",
    "X_test = data_test[:,1:257]\n",
    "y_test = data_test[:,0]\n",
    "\n",
    "# We'll do the same thing on multiple methods: train the method on the training data and then predict on the test data and assess performance\n",
    "\n",
    "# For a quick assessment of prediction performance on the training set, we'll be using the cross validation score (k fold cross-validation k=10)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# For a quick assessment of prediction performance on the test set, we'll be using the accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#To assess the computing cost of each method, we'll time the calculations:\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 1: Logistic regression\n",
    "\n",
    "#Start timer\n",
    "start_lr = timeit.default_timer()\n",
    "\n",
    "#import the logistic regression model from the Scikit-learn library and run the regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "cvs_lr = cross_val_score(lr, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = lr.predict(X_test)\n",
    "as_lr = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_lr = timeit.default_timer()\n",
    "time_lr = stop_lr - start_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 2: Linear discriminant analysis\n",
    "\n",
    "#Start timer\n",
    "start_lda = timeit.default_timer()\n",
    "\n",
    "#import the linear discriminant model from the Scikit-learn library and run the analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train,y_train)\n",
    "\n",
    "cvs_lda = cross_val_score(lda, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = lda.predict(X_test)\n",
    "as_lda = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_lda = timeit.default_timer()\n",
    "time_lda = stop_lda - start_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Quadratic discriminant analysis\n",
    "\n",
    "#Start timer\n",
    "start_qda = timeit.default_timer()\n",
    "\n",
    "#import the quadratic discriminant model from the Scikit-learn library and run the analysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train,y_train)\n",
    "\n",
    "cvs_qda = cross_val_score(qda, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = qda.predict(X_test)\n",
    "as_qda = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_qda = timeit.default_timer()\n",
    "time_qda = stop_qda - start_qda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the quadratic discriminant model warns about potential colinearity in variables, which is normal given the data is composed of 0 & 1 getting squared\n"
     ]
    }
   ],
   "source": [
    "# running the quadratic discriminant model warns about potential colinearity in variables, which is normal given the data is composed of 0 & 1 getting squared\n",
    "print('Running the quadratic discriminant model warns about potential colinearity in variables, which is normal given the data is composed of 0 & 1 getting squared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 4: Bagging classifier\n",
    "\n",
    "#Start timer\n",
    "start_bag = timeit.default_timer()\n",
    "\n",
    "#import the bagging classifier model from the Scikit-learn library and run the classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(n_estimators=100, random_state=1)\n",
    "bag.fit(X_train,y_train)\n",
    "\n",
    "cvs_bag = cross_val_score(bag, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = bag.predict(X_test)\n",
    "as_bag = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_bag = timeit.default_timer()\n",
    "time_bag = stop_bag - start_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 5: ADA boost classifier\n",
    "\n",
    "#Start timer\n",
    "start_adaboost = timeit.default_timer()\n",
    "\n",
    "#import the ADA bagging classifier model from the Scikit-learn library and run the classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
    "adaboost.fit(X_train,y_train)\n",
    "\n",
    "cvs_adaboost = cross_val_score(adaboost, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = adaboost.predict(X_test)\n",
    "as_adaboost = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_adaboost = timeit.default_timer()\n",
    "time_adaboost = stop_adaboost - start_adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 6: Gradient boosting classifier\n",
    "\n",
    "#Start timer\n",
    "start_gradboost = timeit.default_timer()\n",
    "\n",
    "#import the gradient boosting classifier model from the Scikit-learn library and run the classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradboost = GradientBoostingClassifier(n_estimators= 100, random_state=1)\n",
    "gradboost.fit(X_train,y_train)\n",
    "\n",
    "cvs_gradboost = cross_val_score(gradboost, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = gradboost.predict(X_test)\n",
    "as_gradboost = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_gradboost = timeit.default_timer()\n",
    "time_gradboost = stop_gradboost - start_gradboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 7: Random Forest\n",
    "\n",
    "#Start timer\n",
    "start_rf = timeit.default_timer()\n",
    "\n",
    "#import the random forest model from the Scikit-learn library and run the algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "cvs_rf = cross_val_score(rf, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = rf.predict(X_test)\n",
    "as_rf = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_rf = timeit.default_timer()\n",
    "time_rf = stop_rf - start_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 8: Support Vector Machine\n",
    "\n",
    "#Start timer\n",
    "start_sv = timeit.default_timer()\n",
    "\n",
    "#import the support vector machine model from the Scikit-learn library and run the algorithm\n",
    "from sklearn.svm import SVC\n",
    "sv = SVC(C=10)\n",
    "sv.fit(X_train,y_train)\n",
    "\n",
    "cvs_sv = cross_val_score(sv, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test)  \n",
    "y_test_pred = sv.predict(X_test)\n",
    "as_sv = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_sv = timeit.default_timer()\n",
    "time_sv = stop_sv - start_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method 9: Neural network MLP classifier\n",
    "\n",
    "#Start timer\n",
    "start_nn = timeit.default_timer()\n",
    "\n",
    "#import the neural network MLP classifier model from the Scikit-learn library and run the classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(hidden_layer_sizes=(10,10), activation = 'logistic',max_iter=1000)\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "cvs_nn = cross_val_score(nn, X_train, y_train, cv=10).mean()\n",
    "\n",
    "# We now apply our predictive model on the test data (X_test) \n",
    "y_test_pred = nn.predict(X_test)\n",
    "as_nn = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#stop timer and calculate computing time in seconds\n",
    "stop_nn = timeit.default_timer()\n",
    "time_nn = stop_nn - start_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Method  Cross validation score  Accuracy score  \\\n",
      "0              Logistic regression                0.945140        0.911310   \n",
      "1     Linear discriminant analysis                0.921008        0.885401   \n",
      "2  Quadratic discriminant analysis                0.848977        0.813652   \n",
      "3               Bagging classifier                0.941314        0.898356   \n",
      "4             ADA boost classifier                0.683616        0.672147   \n",
      "5     Gradient boosting classifier                0.958998        0.923767   \n",
      "6                    Random Forest                0.967366        0.937220   \n",
      "7           Support Vector Machine                0.979434        0.950174   \n",
      "8    Neural network MLP classifier                0.931015        0.881913   \n",
      "\n",
      "   Computing time (s)  \n",
      "0          144.644628  \n",
      "1            5.264831  \n",
      "2            6.259704  \n",
      "3          807.719380  \n",
      "4          149.690880  \n",
      "5         2090.674100  \n",
      "6           53.433905  \n",
      "7           49.950672  \n",
      "8          163.793673  \n"
     ]
    }
   ],
   "source": [
    "#Creating a pandas dataframe summarizing this first part. First, create a dictionnary with key as the desired section and associated value\n",
    "resume = [('Logistic regression', cvs_lr, as_lr, time_lr),\n",
    "('Linear discriminant analysis', cvs_lda, as_lda, time_lda),\n",
    "('Quadratic discriminant analysis', cvs_qda, as_qda, time_qda),\n",
    "('Bagging classifier', cvs_bag, as_bag, time_bag),\n",
    "('ADA boost classifier', cvs_adaboost, as_adaboost, time_adaboost),\n",
    "('Gradient boosting classifier', cvs_gradboost, as_gradboost, time_gradboost),\n",
    "('Random Forest', cvs_rf, as_rf, time_rf),\n",
    "('Support Vector Machine', cvs_sv, as_sv, time_sv),\n",
    "('Neural network MLP classifier', cvs_nn, as_nn, time_nn),]\n",
    "labels = [ 'Method', 'Cross validation score', 'Accuracy score', 'Computing time (s)']\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(resume, columns=labels)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without being too computer time expensive, the Support Vector Machine, in its default configuration, seems to be the best classifying method with a more than 95% accuracy score.\n"
     ]
    }
   ],
   "source": [
    "#Conclusion: without being too computer time expensive, the Support Vector Machine, in its default configuration, seems to be the best classifying method with a more than 95% accuracy score.\n",
    "print('Without being too computer time expensive, the Support Vector Machine, in its default configuration, seems to be the best classifying method with a more than 95% accuracy score.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
